{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(pos_file_path=None, neg_file_path=None, categorical_columns=None, label_col=\"label\", test_size=0.2, random_state=0):\n",
    "    # Initialize an empty DataFrame for concatenation\n",
    "    data_frames = []\n",
    "    \n",
    "    # Load positive dataset if the path is provided\n",
    "    if pos_file_path is not None:\n",
    "        pos_df = pd.read_csv(pos_file_path)\n",
    "        pos_df[label_col] = 1  # Label positive samples as 1\n",
    "        data_frames.append(pos_df)\n",
    "    \n",
    "    # Load negative dataset if the path is provided\n",
    "    if neg_file_path is not None:\n",
    "        neg_df = pd.read_csv(neg_file_path)\n",
    "        neg_df[label_col] = 0  # Label negative samples as 0\n",
    "        data_frames.append(neg_df)\n",
    "    \n",
    "    # Concatenate available datasets\n",
    "    if not data_frames:\n",
    "        raise ValueError(\"At least one of pos_file_path or neg_file_path must be provided.\")\n",
    "    data_df = pd.concat(data_frames, axis=0)\n",
    "    \n",
    "    # Drop unnecessary columns if present\n",
    "    #if 'dod' in data_df.columns:\n",
    "    #    data_df.drop(columns=['dod'], inplace=True)\n",
    "    \n",
    "    # Split into features and labels\n",
    "    X = data_df.drop(columns=[label_col, \"subject_id\"])\n",
    "    y = data_df[label_col]\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    #if categorical_columns:\n",
    "    #    for col in categorical_columns:\n",
    "    #        le = LabelEncoder()\n",
    "    #        X[col] = le.fit_transform(X[col].astype(str))\n",
    "    \n",
    "    # Fill missing values with column means\n",
    "    #X.fillna(X.mean(), inplace=True)\n",
    "    \n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "# Usage\n",
    "pos_file_path = \"generic_features_for_positive_patient.csv\"\n",
    "neg_file_path = \"generic_features_for_random_patient.csv\"\n",
    "CAT_COLUMN = ['race', 'marital_status', 'gender', 'dod']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = preprocess_data(pos_file_path, neg_file_path, CAT_COLUMN, label_col=\"label\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Steven\\anaconda3\\envs\\CV_project\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69196 | val_0_auc: 0.70438 |  0:00:00s\n",
      "epoch 1  | loss: 0.62365 | val_0_auc: 0.80786 |  0:00:00s\n",
      "epoch 2  | loss: 0.53319 | val_0_auc: 0.85587 |  0:00:00s\n",
      "epoch 3  | loss: 0.46638 | val_0_auc: 0.88118 |  0:00:00s\n",
      "epoch 4  | loss: 0.42848 | val_0_auc: 0.90671 |  0:00:00s\n",
      "epoch 5  | loss: 0.39929 | val_0_auc: 0.91792 |  0:00:01s\n",
      "epoch 6  | loss: 0.37911 | val_0_auc: 0.92297 |  0:00:01s\n",
      "epoch 7  | loss: 0.36739 | val_0_auc: 0.92964 |  0:00:01s\n",
      "epoch 8  | loss: 0.35244 | val_0_auc: 0.93207 |  0:00:01s\n",
      "epoch 9  | loss: 0.34683 | val_0_auc: 0.93797 |  0:00:01s\n",
      "epoch 10 | loss: 0.34143 | val_0_auc: 0.93778 |  0:00:02s\n",
      "epoch 11 | loss: 0.33991 | val_0_auc: 0.94058 |  0:00:02s\n",
      "epoch 12 | loss: 0.33441 | val_0_auc: 0.94157 |  0:00:02s\n",
      "epoch 13 | loss: 0.32426 | val_0_auc: 0.94624 |  0:00:02s\n",
      "epoch 14 | loss: 0.31866 | val_0_auc: 0.94561 |  0:00:02s\n",
      "epoch 15 | loss: 0.31253 | val_0_auc: 0.94507 |  0:00:03s\n",
      "epoch 16 | loss: 0.30366 | val_0_auc: 0.94529 |  0:00:03s\n",
      "epoch 17 | loss: 0.30143 | val_0_auc: 0.94732 |  0:00:03s\n",
      "epoch 18 | loss: 0.29986 | val_0_auc: 0.95231 |  0:00:03s\n",
      "epoch 19 | loss: 0.28432 | val_0_auc: 0.95538 |  0:00:03s\n",
      "epoch 20 | loss: 0.28548 | val_0_auc: 0.95564 |  0:00:04s\n",
      "epoch 21 | loss: 0.2825  | val_0_auc: 0.95206 |  0:00:04s\n",
      "epoch 22 | loss: 0.2834  | val_0_auc: 0.95444 |  0:00:04s\n",
      "epoch 23 | loss: 0.28493 | val_0_auc: 0.95538 |  0:00:04s\n",
      "epoch 24 | loss: 0.27839 | val_0_auc: 0.95661 |  0:00:04s\n",
      "epoch 25 | loss: 0.27079 | val_0_auc: 0.9581  |  0:00:05s\n",
      "epoch 26 | loss: 0.27269 | val_0_auc: 0.95716 |  0:00:05s\n",
      "epoch 27 | loss: 0.27672 | val_0_auc: 0.95803 |  0:00:05s\n",
      "epoch 28 | loss: 0.27204 | val_0_auc: 0.95627 |  0:00:05s\n",
      "epoch 29 | loss: 0.26613 | val_0_auc: 0.95411 |  0:00:05s\n",
      "epoch 30 | loss: 0.26811 | val_0_auc: 0.9543  |  0:00:06s\n",
      "epoch 31 | loss: 0.26126 | val_0_auc: 0.95702 |  0:00:06s\n",
      "epoch 32 | loss: 0.25321 | val_0_auc: 0.95885 |  0:00:06s\n",
      "epoch 33 | loss: 0.24872 | val_0_auc: 0.95767 |  0:00:06s\n",
      "epoch 34 | loss: 0.24544 | val_0_auc: 0.95998 |  0:00:06s\n",
      "epoch 35 | loss: 0.25087 | val_0_auc: 0.95881 |  0:00:06s\n",
      "epoch 36 | loss: 0.24365 | val_0_auc: 0.95856 |  0:00:07s\n",
      "epoch 37 | loss: 0.24241 | val_0_auc: 0.95826 |  0:00:07s\n",
      "epoch 38 | loss: 0.23817 | val_0_auc: 0.96036 |  0:00:07s\n",
      "epoch 39 | loss: 0.23415 | val_0_auc: 0.96097 |  0:00:07s\n",
      "epoch 40 | loss: 0.24068 | val_0_auc: 0.96045 |  0:00:07s\n",
      "epoch 41 | loss: 0.23839 | val_0_auc: 0.9596  |  0:00:08s\n",
      "epoch 42 | loss: 0.23481 | val_0_auc: 0.95892 |  0:00:08s\n",
      "epoch 43 | loss: 0.22685 | val_0_auc: 0.9605  |  0:00:08s\n",
      "epoch 44 | loss: 0.23392 | val_0_auc: 0.95996 |  0:00:08s\n",
      "epoch 45 | loss: 0.22664 | val_0_auc: 0.95943 |  0:00:08s\n",
      "epoch 46 | loss: 0.22771 | val_0_auc: 0.95743 |  0:00:09s\n",
      "epoch 47 | loss: 0.22164 | val_0_auc: 0.95633 |  0:00:09s\n",
      "epoch 48 | loss: 0.22081 | val_0_auc: 0.95692 |  0:00:09s\n",
      "epoch 49 | loss: 0.21376 | val_0_auc: 0.95714 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 39 and best_val_0_auc = 0.96097\n",
      "Validation Accuracy: 88.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Steven\\anaconda3\\envs\\CV_project\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize TabNet model\n",
    "model = TabNetClassifier()\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    batch_size=1024, \n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_valid)\n",
    "\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8503611971104231\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_valid)\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_valid)\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race: 0.0021550639560379108\n",
      "marital_status: 0.07263022284860916\n",
      "age: 0.1935844041026247\n",
      "average_apsiii: 0.02673013617234582\n",
      "avg_charlson_comorbidity_index: 0.03169528516412599\n",
      "gender: 0.054589863364509376\n",
      "dod: 0.02127761062733792\n",
      "avg_ph: 0.20356743019347365\n",
      "avg_body_weight: 0.04065667136613818\n",
      "BMI: 0.009111778991516951\n",
      "height_inches: 0.052398351611021826\n",
      "average_los_icu: 5.025734337689912e-06\n",
      "avg_glucose: 0.02107819875087031\n",
      "avg_heart_rate: 0.03754229765893099\n",
      "avg_mbp: 0.0015110488155761698\n",
      "avg_resp_rate: 0.015965659398276894\n",
      "avg_spo2: 0.0017490473816516216\n",
      "avg_temperature: 0.11976966828198839\n",
      "avg_systolic_blood_pressure: 0.02066167261337099\n",
      "avg_diastolic_blood_pressure: 0.07332056296725535\n"
     ]
    }
   ],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv(\"generic_features_for_positive_patient.csv\")\n",
    "\n",
    "# Get column names\n",
    "columns = df.columns.drop(\"subject_id\")\n",
    "\n",
    "\n",
    "# Pair feature importances with column names\n",
    "feature_importance_dict = dict(zip(columns, feature_importances))\n",
    "\n",
    "# Display the feature importance for each column\n",
    "for column, importance in feature_importance_dict.items():\n",
    "    print(f\"{column}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Steven\\anaconda3\\envs\\CV_project\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.7419  | val_0_auc: 0.56109 |  0:00:00s\n",
      "epoch 1  | loss: 0.69035 | val_0_auc: 0.55941 |  0:00:00s\n",
      "epoch 2  | loss: 0.66765 | val_0_auc: 0.61374 |  0:00:00s\n",
      "epoch 3  | loss: 0.65481 | val_0_auc: 0.65047 |  0:00:00s\n",
      "epoch 4  | loss: 0.64501 | val_0_auc: 0.65563 |  0:00:00s\n",
      "epoch 5  | loss: 0.62571 | val_0_auc: 0.68999 |  0:00:00s\n",
      "epoch 6  | loss: 0.61982 | val_0_auc: 0.70128 |  0:00:01s\n",
      "epoch 7  | loss: 0.62219 | val_0_auc: 0.71007 |  0:00:01s\n",
      "epoch 8  | loss: 0.61772 | val_0_auc: 0.70687 |  0:00:01s\n",
      "epoch 9  | loss: 0.60666 | val_0_auc: 0.71625 |  0:00:01s\n",
      "epoch 10 | loss: 0.5986  | val_0_auc: 0.71277 |  0:00:01s\n",
      "epoch 11 | loss: 0.58132 | val_0_auc: 0.71887 |  0:00:01s\n",
      "epoch 12 | loss: 0.59513 | val_0_auc: 0.71843 |  0:00:01s\n",
      "epoch 13 | loss: 0.57907 | val_0_auc: 0.71639 |  0:00:02s\n",
      "epoch 14 | loss: 0.58078 | val_0_auc: 0.71272 |  0:00:02s\n",
      "epoch 15 | loss: 0.58746 | val_0_auc: 0.72484 |  0:00:02s\n",
      "epoch 16 | loss: 0.58266 | val_0_auc: 0.74366 |  0:00:02s\n",
      "epoch 17 | loss: 0.56834 | val_0_auc: 0.75231 |  0:00:02s\n",
      "epoch 18 | loss: 0.57294 | val_0_auc: 0.75372 |  0:00:02s\n",
      "epoch 19 | loss: 0.56071 | val_0_auc: 0.75396 |  0:00:02s\n",
      "epoch 20 | loss: 0.55988 | val_0_auc: 0.75076 |  0:00:03s\n",
      "epoch 21 | loss: 0.56256 | val_0_auc: 0.74858 |  0:00:03s\n",
      "epoch 22 | loss: 0.54838 | val_0_auc: 0.74941 |  0:00:03s\n",
      "epoch 23 | loss: 0.55974 | val_0_auc: 0.74622 |  0:00:03s\n",
      "epoch 24 | loss: 0.55773 | val_0_auc: 0.74932 |  0:00:03s\n",
      "epoch 25 | loss: 0.55837 | val_0_auc: 0.74831 |  0:00:03s\n",
      "epoch 26 | loss: 0.54487 | val_0_auc: 0.74871 |  0:00:03s\n",
      "epoch 27 | loss: 0.55167 | val_0_auc: 0.74793 |  0:00:04s\n",
      "epoch 28 | loss: 0.55732 | val_0_auc: 0.7509  |  0:00:04s\n",
      "epoch 29 | loss: 0.53847 | val_0_auc: 0.75314 |  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_auc = 0.75396\n",
      "Validation Accuracy: 68.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Steven\\anaconda3\\envs\\CV_project\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Freeze all layers except the last layer for fine-tuning\n",
    "for name, param in model.network.named_parameters():\n",
    "    # Freeze all parameters except those in the last block\n",
    "    if \"blocks\" in name and not name.startswith(\"network.blocks.0\"):\n",
    "        param.requires_grad = True  # Unfreeze the last block\n",
    "    else:\n",
    "        param.requires_grad = False  # Freeze earlier layers\n",
    "\n",
    "'''\n",
    "\n",
    "# Usage\n",
    "fine_tune_file_path = \"negative_generic_features.csv\"\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = preprocess_data(pos_file_path, fine_tune_file_path, CAT_COLUMN, label_col=\"label\")\n",
    "\n",
    "# Initialize TabNet model\n",
    "model = TabNetClassifier()\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    max_epochs=50, \n",
    "    patience=10,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_valid)\n",
    "\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
